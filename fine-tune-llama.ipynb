{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7016635,
     "sourceType": "datasetVersion",
     "datasetId": 4006737,
     "isSourceIdPinned": true
    }
   ],
   "dockerImageVersionId": 30580,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "%%capture\n!pip install \"numpy>=1.12.1\"\n!pip install \"scipy>=1.0.1\"\n!pip install \"torch>=2.0.0\"\n!pip install accelerate==0.23.0\n# !pip install bitsandbytes==0.41.0\n!pip install bitsandbytes\n!pip install peft==0.5.0\n!pip install transformers==4.34.0\n!pip install pillow==10.0.1\n!pip install llama-cpp-python\n!pip install datasets\n!pip install zstandard\n!pip install jsonlines\n!pip install sentencepiece\n!pip install fire\n!pip install datasketch==1.5.9\n!pip install nltk==3.8.1\n!pip install scikit-learn==1.3.0\n!pip install pytest",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:44:41.351022Z",
     "iopub.execute_input": "2023-11-28T08:44:41.351374Z",
     "iopub.status.idle": "2023-11-28T08:49:24.181416Z",
     "shell.execute_reply.started": "2023-11-28T08:44:41.351341Z",
     "shell.execute_reply": "2023-11-28T08:49:24.180035Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T19:58:32.920352Z",
     "start_time": "2024-07-04T19:57:49.306196Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import os\nos._exit(00) # Перезапустить ядро",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-07-03T12:03:36.689891Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "%mkdir /tmp\n",
    "%ls"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:49:59.773664Z",
     "iopub.execute_input": "2023-11-28T08:49:59.774325Z",
     "iopub.status.idle": "2023-11-28T08:50:01.646347Z",
     "shell.execute_reply.started": "2023-11-28T08:49:59.774293Z",
     "shell.execute_reply": "2023-11-28T08:50:01.645235Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T20:07:26.311293Z",
     "start_time": "2024-07-03T20:07:26.228818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ЋиЁЎЄ  ў бЁ­в ЄбЁбҐ Є®¬ ­¤л.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ E ­Ґ Ё¬ҐҐв ¬ҐвЄЁ.\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : C884-71E7\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\n",
      "\n",
      "03.07.2024  23:07    <DIR>          .\n",
      "03.07.2024  23:07    <DIR>          ..\n",
      "03.07.2024  17:27    <DIR>          .idea\n",
      "03.07.2024  12:42            22я395 extraction-saiga.ipynb\n",
      "03.07.2024  23:07           277я848 fine-tune-llama.ipynb\n",
      "03.07.2024  14:59    <DIR>          venv\n",
      "               2 д ©«®ў        300я243 Ў ©в\n",
      "               4 Ї Ї®Є  342я691я061я760 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/ggerganov/llama.cpp.git\n!git clone https://github.com/IlyaGusev/rulm.git",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:50:01.648178Z",
     "iopub.execute_input": "2023-11-28T08:50:01.648482Z",
     "iopub.status.idle": "2023-11-28T08:50:10.518203Z",
     "shell.execute_reply.started": "2023-11-28T08:50:01.648455Z",
     "shell.execute_reply": "2023-11-28T08:50:10.517100Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T20:11:14.186424Z",
     "start_time": "2024-07-03T20:07:37.257165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Cloning into 'rulm'...\n",
      "Updating files:  33% (118/354)\n",
      "Updating files:  34% (121/354)\n",
      "Updating files:  35% (124/354)\n",
      "Updating files:  36% (128/354)\n",
      "Updating files:  37% (131/354)\n",
      "Updating files:  38% (135/354)\n",
      "Updating files:  39% (139/354)\n",
      "Updating files:  40% (142/354)\n",
      "Updating files:  41% (146/354)\n",
      "Updating files:  42% (149/354)\n",
      "Updating files:  43% (153/354)\n",
      "Updating files:  44% (156/354)\n",
      "Updating files:  45% (160/354)\n",
      "Updating files:  46% (163/354)\n",
      "Updating files:  47% (167/354)\n",
      "Updating files:  48% (170/354)\n",
      "Updating files:  49% (174/354)\n",
      "Updating files:  50% (177/354)\n",
      "Updating files:  51% (181/354)\n",
      "Updating files:  52% (185/354)\n",
      "Updating files:  53% (188/354)\n",
      "Updating files:  54% (192/354)\n",
      "Updating files:  55% (195/354)\n",
      "Updating files:  56% (199/354)\n",
      "Updating files:  57% (202/354)\n",
      "Updating files:  58% (206/354)\n",
      "Updating files:  59% (209/354)\n",
      "Updating files:  60% (213/354)\n",
      "Updating files:  61% (216/354)\n",
      "Updating files:  62% (220/354)\n",
      "Updating files:  63% (224/354)\n",
      "Updating files:  64% (227/354)\n",
      "Updating files:  65% (231/354)\n",
      "Updating files:  66% (234/354)\n",
      "Updating files:  67% (238/354)\n",
      "Updating files:  68% (241/354)\n",
      "Updating files:  69% (245/354)\n",
      "Updating files:  70% (248/354)\n",
      "Updating files:  71% (252/354)\n",
      "Updating files:  72% (255/354)\n",
      "Updating files:  73% (259/354)\n",
      "Updating files:  74% (262/354)\n",
      "Updating files:  75% (266/354)\n",
      "Updating files:  76% (270/354)\n",
      "Updating files:  77% (273/354)\n",
      "Updating files:  78% (277/354)\n",
      "Updating files:  79% (280/354)\n",
      "Updating files:  80% (284/354)\n",
      "Updating files:  81% (287/354)\n",
      "Updating files:  82% (291/354)\n",
      "Updating files:  83% (294/354)\n",
      "Updating files:  84% (298/354)\n",
      "Updating files:  85% (301/354)\n",
      "Updating files:  86% (305/354)\n",
      "Updating files:  87% (308/354)\n",
      "Updating files:  88% (312/354)\n",
      "Updating files:  89% (316/354)\n",
      "Updating files:  90% (319/354)\n",
      "Updating files:  91% (323/354)\n",
      "Updating files:  92% (326/354)\n",
      "Updating files:  93% (330/354)\n",
      "Updating files:  94% (333/354)\n",
      "Updating files:  95% (337/354)\n",
      "Updating files:  96% (340/354)\n",
      "Updating files:  97% (344/354)\n",
      "Updating files:  98% (347/354)\n",
      "Updating files:  99% (351/354)\n",
      "Updating files: 100% (354/354)\n",
      "Updating files: 100% (354/354), done.\n",
      "Filtering content:   1% (2/120)\n",
      "Filtering content:   2% (3/120), 3.56 MiB | 735.00 KiB/s\n",
      "Filtering content:   3% (4/120), 6.27 MiB | 743.00 KiB/s\n",
      "Filtering content:   4% (5/120), 7.67 MiB | 781.00 KiB/s\n",
      "Filtering content:   5% (6/120), 10.86 MiB | 1.03 MiB/s \n",
      "Filtering content:   5% (7/120), 10.86 MiB | 1.03 MiB/s\n",
      "Filtering content:   6% (8/120), 15.47 MiB | 1.38 MiB/s\n",
      "Filtering content:   7% (9/120), 18.97 MiB | 1.61 MiB/s\n",
      "Filtering content:   8% (10/120), 18.97 MiB | 1.61 MiB/s\n",
      "Filtering content:   9% (11/120), 23.73 MiB | 1.71 MiB/s\n",
      "Filtering content:  10% (12/120), 24.85 MiB | 1.63 MiB/s\n",
      "Filtering content:  11% (14/120), 25.95 MiB | 1.63 MiB/s\n",
      "Filtering content:  12% (15/120), 28.12 MiB | 1.87 MiB/s\n",
      "Filtering content:  13% (16/120), 29.20 MiB | 2.30 MiB/s\n",
      "Filtering content:  14% (17/120), 29.20 MiB | 2.30 MiB/s\n",
      "Filtering content:  15% (18/120), 31.36 MiB | 2.30 MiB/s\n",
      "Filtering content:  15% (19/120), 32.42 MiB | 1.98 MiB/s\n",
      "Filtering content:  16% (20/120), 32.42 MiB | 1.98 MiB/s\n",
      "Filtering content:  17% (21/120), 34.50 MiB | 1.79 MiB/s\n",
      "Filtering content:  18% (22/120), 35.52 MiB | 1.59 MiB/s\n",
      "Filtering content:  19% (23/120), 36.53 MiB | 1.45 MiB/s\n",
      "Filtering content:  20% (24/120), 36.53 MiB | 1.45 MiB/s\n",
      "Filtering content:  20% (25/120), 38.50 MiB | 1.48 MiB/s\n",
      "Filtering content:  21% (26/120), 39.49 MiB | 1.37 MiB/s\n",
      "Filtering content:  22% (27/120), 40.47 MiB | 1.30 MiB/s\n",
      "Filtering content:  23% (28/120), 41.44 MiB | 1.29 MiB/s\n",
      "Filtering content:  24% (29/120), 42.41 MiB | 1.35 MiB/s\n",
      "Filtering content:  25% (30/120), 42.41 MiB | 1.35 MiB/s\n",
      "Filtering content:  25% (31/120), 42.41 MiB | 1.35 MiB/s\n",
      "Filtering content:  26% (32/120), 45.29 MiB | 1.50 MiB/s\n",
      "Filtering content:  27% (33/120), 46.25 MiB | 1.29 MiB/s\n",
      "Filtering content:  28% (34/120), 47.20 MiB | 1.25 MiB/s\n",
      "Filtering content:  29% (35/120), 47.20 MiB | 1.25 MiB/s\n",
      "Filtering content:  30% (36/120), 49.12 MiB | 1.31 MiB/s\n",
      "Filtering content:  31% (38/120), 51.02 MiB | 1.48 MiB/s\n",
      "Filtering content:  32% (39/120), 51.97 MiB | 1.45 MiB/s\n",
      "Filtering content:  33% (40/120), 52.92 MiB | 1.36 MiB/s\n",
      "Filtering content:  34% (41/120), 53.86 MiB | 1.29 MiB/s\n",
      "Filtering content:  35% (42/120), 53.86 MiB | 1.29 MiB/s\n",
      "Filtering content:  36% (44/120), 55.75 MiB | 1.37 MiB/s\n",
      "Filtering content:  37% (45/120), 57.61 MiB | 1.46 MiB/s\n",
      "Filtering content:  38% (46/120), 58.54 MiB | 1.42 MiB/s\n",
      "Filtering content:  39% (47/120), 59.47 MiB | 1.41 MiB/s\n",
      "Filtering content:  40% (48/120), 60.40 MiB | 1.27 MiB/s\n",
      "Filtering content:  41% (50/120), 61.30 MiB | 1.13 MiB/s\n",
      "Filtering content:  42% (51/120), 61.30 MiB | 1.13 MiB/s\n",
      "Filtering content:  43% (52/120), 61.30 MiB | 1.13 MiB/s\n",
      "Filtering content:  44% (53/120), 64.80 MiB | 1.34 MiB/s\n",
      "Filtering content:  45% (54/120), 64.80 MiB | 1.34 MiB/s\n",
      "Filtering content:  45% (55/120), 66.44 MiB | 1.50 MiB/s\n",
      "Filtering content:  46% (56/120), 67.17 MiB | 1.52 MiB/s\n",
      "Filtering content:  47% (57/120), 67.17 MiB | 1.52 MiB/s\n",
      "Filtering content:  48% (58/120), 67.95 MiB | 1.42 MiB/s\n",
      "Filtering content:  49% (59/120), 67.95 MiB | 1.42 MiB/s\n",
      "Filtering content:  50% (60/120), 67.95 MiB | 1.42 MiB/s\n",
      "Filtering content:  51% (62/120), 67.95 MiB | 1.42 MiB/s\n",
      "Filtering content:  52% (63/120), 67.95 MiB | 1.42 MiB/s\n",
      "Filtering content:  53% (64/120), 70.57 MiB | 1.47 MiB/s\n",
      "Filtering content:  54% (65/120), 70.57 MiB | 1.47 MiB/s\n",
      "Filtering content:  55% (66/120), 70.57 MiB | 1.47 MiB/s\n",
      "Filtering content:  55% (67/120), 70.57 MiB | 1.47 MiB/s\n",
      "Filtering content:  56% (68/120), 71.25 MiB | 1.63 MiB/s\n",
      "Filtering content:  57% (69/120), 71.25 MiB | 1.63 MiB/s\n",
      "Filtering content:  58% (70/120), 71.25 MiB | 1.63 MiB/s\n",
      "Filtering content:  59% (71/120), 71.25 MiB | 1.63 MiB/s\n",
      "Filtering content:  60% (72/120), 71.25 MiB | 1.63 MiB/s\n",
      "Filtering content:  60% (73/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  61% (74/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  62% (75/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  63% (76/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  64% (77/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  65% (78/120), 72.03 MiB | 1.58 MiB/s\n",
      "Filtering content:  66% (80/120), 73.04 MiB | 1.61 MiB/s\n",
      "Filtering content:  67% (81/120), 73.04 MiB | 1.61 MiB/s\n",
      "Filtering content:  68% (82/120), 73.04 MiB | 1.61 MiB/s\n",
      "Filtering content:  69% (83/120), 73.04 MiB | 1.61 MiB/s\n",
      "Filtering content:  70% (84/120), 73.04 MiB | 1.61 MiB/s\n",
      "Filtering content:  71% (86/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  72% (87/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  73% (88/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  74% (89/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  75% (90/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  76% (92/120), 73.69 MiB | 1.46 MiB/s\n",
      "Filtering content:  77% (93/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  78% (94/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  79% (95/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  80% (96/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  81% (98/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  82% (99/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  83% (100/120), 74.50 MiB | 1.36 MiB/s\n",
      "Filtering content:  84% (101/120), 74.97 MiB | 1.19 MiB/s\n",
      "Filtering content:  84% (101/120), 116.32 MiB | 1.87 MiB/s\n",
      "Filtering content:  85% (102/120), 116.32 MiB | 1.87 MiB/s\n",
      "Filtering content:  86% (104/120), 116.49 MiB | 1.80 MiB/s\n",
      "Filtering content:  87% (105/120), 116.49 MiB | 1.80 MiB/s\n",
      "Filtering content:  88% (106/120), 116.49 MiB | 1.80 MiB/s\n",
      "Filtering content:  89% (107/120), 116.49 MiB | 1.80 MiB/s\n",
      "Filtering content:  90% (108/120), 116.49 MiB | 1.80 MiB/s\n",
      "Filtering content:  91% (110/120), 117.73 MiB | 1.76 MiB/s\n",
      "Filtering content:  92% (111/120), 117.73 MiB | 1.76 MiB/s\n",
      "Filtering content:  93% (112/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  94% (113/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  95% (114/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  96% (116/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  97% (117/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  98% (118/120), 118.02 MiB | 1.75 MiB/s\n",
      "Filtering content:  99% (119/120), 118.99 MiB | 1.77 MiB/s\n",
      "Filtering content: 100% (120/120), 118.99 MiB | 1.77 MiB/s\n",
      "Filtering content: 100% (120/120), 244.31 MiB | 1.49 MiB/s, done.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport pandas as pd\nfrom peft import PeftModel, PeftConfig\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForTokenClassification, AutoConfig, GenerationConfig\nfrom transformers import Trainer, TrainingArguments, logging, TrainerCallback, TrainerState, TrainerControl, BitsAndBytesConfig\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nimport time\nfrom typing import Any, List, Mapping, Optional\nimport transformers\nimport os\nfrom pathlib import Path",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:50:10.519891Z",
     "iopub.execute_input": "2023-11-28T08:50:10.520802Z",
     "iopub.status.idle": "2023-11-28T08:50:26.609253Z",
     "shell.execute_reply.started": "2023-11-28T08:50:10.520763Z",
     "shell.execute_reply": "2023-11-28T08:50:26.608441Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T19:51:33.678801Z",
     "start_time": "2024-07-04T19:51:33.047644Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1282\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1281\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1206\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1178\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1149\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\trainer.py:168\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_datasets_available():\n\u001B[1;32m--> 168\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_tpu_available(check_device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\__init__.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.20.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:76\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowReader\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_writer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowWriter, OptimizedTypedSequence\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py:34\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaming\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _split_re, filenames_for_dataset_split\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InMemoryTable, MemoryMappedTable, Table, concat_tables\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logging\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\table.py:13\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_logger\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\utils\\__init__.py:17\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minfo_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VerificationMode\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m disable_progress_bar, enable_progress_bar, is_progress_bar_enabled\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\datasets\\utils\\info_utils.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optional\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m insecure_hashlib\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoPeftModelForCausalLM\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForCausalLM, DataCollatorForTokenClassification, AutoConfig, GenerationConfig\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer, TrainingArguments, logging, TrainerCallback, TrainerState, TrainerControl, BitsAndBytesConfig\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PREFIX_CHECKPOINT_DIR\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1231\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1272\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1270\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m   1271\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1272\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1273\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1284\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1283\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1285\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1286\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1287\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nst_time = time.time()\n\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype= torch.float16,\n#     bnb_4bit_use_double_quant=False,\n# )\n\n\nconfig = PeftConfig.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    load_in_8bit = True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(\n    model,\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    is_trainable = True\n).to(device)\n\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\nprint(generation_config)\nprint(f'Прошло времени {time.time() - st_time}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:50:26.612186Z",
     "iopub.execute_input": "2023-11-28T08:50:26.613281Z",
     "iopub.status.idle": "2023-11-28T08:52:39.000713Z",
     "shell.execute_reply.started": "2023-11-28T08:50:26.613243Z",
     "shell.execute_reply": "2023-11-28T08:52:38.999679Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T22:12:53.515656Z",
     "start_time": "2024-07-03T22:12:51.822736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "224d36d5c0aa4bd78795d58e350988e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--IlyaGusev--saiga2_7b_lora. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a115354bd23e46cdb2d385b147381867"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-fp16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 15\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#     load_in_4bit=True,\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m#     bnb_4bit_quant_type=\"nf4\",\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#     bnb_4bit_compute_dtype= torch.float16,\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m#     bnb_4bit_use_double_quant=False,\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m     14\u001B[0m config \u001B[38;5;241m=\u001B[39m PeftConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(MODEL_NAME)\n\u001B[1;32m---> 15\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_8bit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     20\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m model \u001B[38;5;241m=\u001B[39m PeftModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     22\u001B[0m     model,\n\u001B[0;32m     23\u001B[0m     MODEL_NAME,\n\u001B[0;32m     24\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m     25\u001B[0m     is_trainable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     26\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     28\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    563\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    564\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 565\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    571\u001B[0m )\n",
      "File \u001B[1;32mE:\\PyCharm Community Edition 2021.1.1\\projects\\LLM_LoRa\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2614\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2612\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_in_8bit \u001B[38;5;129;01mor\u001B[39;00m load_in_4bit:\n\u001B[0;32m   2613\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_accelerate_available() \u001B[38;5;129;01mand\u001B[39;00m is_bitsandbytes_available()):\n\u001B[1;32m-> 2614\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   2615\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2616\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2617\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m pip install bitsandbytes` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2618\u001B[0m         )\n\u001B[0;32m   2620\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2621\u001B[0m         \u001B[38;5;66;03m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001B[39;00m\n\u001B[0;32m   2622\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2623\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverriding torch_dtype=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch_dtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with `torch_dtype=torch.float16` due to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2624\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2625\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2626\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m torch_dtype=torch.float16 to remove this warning.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2627\u001B[0m         )\n",
      "\u001B[1;31mImportError\u001B[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.001946Z",
     "iopub.execute_input": "2023-11-28T08:52:39.002212Z",
     "iopub.status.idle": "2023-11-28T08:52:39.008312Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.002189Z",
     "shell.execute_reply": "2023-11-28T08:52:39.007395Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "config",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.009553Z",
     "iopub.execute_input": "2023-11-28T08:52:39.009888Z",
     "iopub.status.idle": "2023-11-28T08:52:39.023726Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.009836Z",
     "shell.execute_reply": "2023-11-28T08:52:39.022794Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='TheBloke/Llama-2-7B-fp16', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print_trainable_parameters(model)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.025033Z",
     "iopub.execute_input": "2023-11-28T08:52:39.025456Z",
     "iopub.status.idle": "2023-11-28T08:52:39.042203Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.025424Z",
     "shell.execute_reply": "2023-11-28T08:52:39.041232Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable params: 16777216 || all params: 6755192832 || trainable%: 0.24836028248556738\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Загружаем датасет",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import load_dataset",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.043674Z",
     "iopub.execute_input": "2023-11-28T08:52:39.044020Z",
     "iopub.status.idle": "2023-11-28T08:52:39.051650Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.043989Z",
     "shell.execute_reply": "2023-11-28T08:52:39.050861Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = load_dataset(\n    \"json\", \n    data_files={'train' : '/kaggle/input/fine-tunning-llama/train.json' , 'validation' : '/kaggle/input/fine-tunning-llama/val.json'}\n)\n\ndata",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.054838Z",
     "iopub.execute_input": "2023-11-28T08:52:39.055142Z",
     "iopub.status.idle": "2023-11-28T08:52:39.596636Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.055118Z",
     "shell.execute_reply": "2023-11-28T08:52:39.595711Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b9460e3075a3c6fd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ded5d046aa4430aa3284e1a523c241f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "249d7b96fd2e49d9a6038ee580caf450"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b9460e3075a3c6fd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c3ff9f4e764b52ab1d9a775853c987"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['system', 'user', 'bot'],\n        num_rows: 22\n    })\n    validation: Dataset({\n        features: ['system', 'user', 'bot'],\n        num_rows: 6\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Предобработка датасета",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(tokenizer.bos_token_id)\nprint(tokenizer.eos_token_id)\nprint(tokenizer.decode([1]))\nprint(tokenizer.decode([2]))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.600893Z",
     "iopub.execute_input": "2023-11-28T08:52:39.601535Z",
     "iopub.status.idle": "2023-11-28T08:52:39.607673Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.601499Z",
     "shell.execute_reply": "2023-11-28T08:52:39.606706Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "1\n2\n<s>\n</s>\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print(tokenizer('system', add_special_tokens=False))\nprint(tokenizer('user', add_special_tokens=False))\nprint(tokenizer('bot', add_special_tokens=False))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.608704Z",
     "iopub.execute_input": "2023-11-28T08:52:39.608984Z",
     "iopub.status.idle": "2023-11-28T08:52:39.658944Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.608960Z",
     "shell.execute_reply": "2023-11-28T08:52:39.658017Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [1788], 'attention_mask': [1]}\n{'input_ids': [1404], 'attention_mask': [1]}\n{'input_ids': [9225], 'attention_mask': [1]}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "tokenizer('<s>system привет как дела</s><s>',truncation=True,\n        max_length=3584,\n        padding=False,\n        return_tensors=None)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.660088Z",
     "iopub.execute_input": "2023-11-28T08:52:39.660419Z",
     "iopub.status.idle": "2023-11-28T08:52:39.670505Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.660382Z",
     "shell.execute_reply": "2023-11-28T08:52:39.669711Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': [1, 1, 1788, 1695, 7616, 5413, 24280, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "#  promt = f\"\"\"<s>system\n# {data_point['system']}</s><s>user\n# {data_point['user']}</s><s>bot\n# {data_point['bot']}</s>\n#     \"\"\"\n    #     print(promt)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.671540Z",
     "iopub.execute_input": "2023-11-28T08:52:39.671796Z",
     "iopub.status.idle": "2023-11-28T08:52:39.682721Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.671773Z",
     "shell.execute_reply": "2023-11-28T08:52:39.681888Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "CUTOFF_LEN = 3584\n\nSYSTEM = tokenizer('system', add_special_tokens=False)['input_ids'][0]\nUSER = tokenizer('user', add_special_tokens=False)['input_ids'][0]\nBOT = tokenizer('bot', add_special_tokens=False)['input_ids'][0]\nprint(f'system: {SYSTEM}')\nprint(f'user: {USER}')\nprint(f'bot: {BOT}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.683894Z",
     "iopub.execute_input": "2023-11-28T08:52:39.684179Z",
     "iopub.status.idle": "2023-11-28T08:52:39.694453Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.684156Z",
     "shell.execute_reply": "2023-11-28T08:52:39.693372Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "system: 1788\nuser: 1404\nbot: 9225\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "\n# def generate_prompt(data_point):\n#     tokenizer_full_promt = {'input_ids': [], 'attention_mask': [], 'labels': []}\n#     for key in data_point.keys():\n#         tokenizer_promt = tokenize(data_point[key])\n#         if key == 'system':\n#             tokenizer_full_promt['input_ids'].append(1)\n#             tokenizer_full_promt['input_ids'].append(SYSTEM)\n#             tokenizer_full_promt['labels'].append(1)\n#             tokenizer_full_promt['labels'].append(SYSTEM)\n#             tokenizer_full_promt['attention_mask'] += [1, 1]\n#         elif key == 'user':\n#             tokenizer_full_promt['input_ids'].append(1)\n#             tokenizer_full_promt['input_ids'].append(USER)\n#             tokenizer_full_promt['labels'].append(1)\n#             tokenizer_full_promt['labels'].append(USER)\n#             tokenizer_full_promt['attention_mask'] += [1, 1]\n    \n#         elif key == 'bot':\n#             tokenizer_full_promt['input_ids'].append(1)\n#             tokenizer_full_promt['input_ids'].append(BOT)\n#             tokenizer_full_promt['labels'].append(1)\n#             tokenizer_full_promt['labels'].append(BOT)\n#             tokenizer_full_promt['attention_mask'] += [1, 1]\n            \n#         else:\n#             print('Что то пошло не так')\n            \n#         for key_promt in tokenizer_promt.keys():\n#             tokenizer_full_promt[key_promt] += tokenizer_promt[key_promt]\n        \n#         tokenizer_full_promt['input_ids'].append(2)\n#         tokenizer_full_promt['labels'].append(2)\n#         tokenizer_full_promt['attention_mask'].append(1)\n        \n        \n#     return tokenizer_full_promt\n \n    \n# def tokenize (prompt, add_eos_token=False):\n#     result = tokenizer(\n#         prompt,\n#         truncation=True,\n#         max_length=CUTOFF_LEN,\n#         padding=False,\n#         return_tensors=None,\n#         add_special_tokens=False\n#     )\n#     if (\n#         result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < CUTOFF_LEN\n#         and add_eos_token\n#     ):\n        \n#         result[\"input_ids\"].append(tokenizer.eos_token_id)\n#         result[\"attention_mask\"].append(1)\n        \n        \n    \n#     result[\"labels\"] = result[\"input_ids\"].copy()\n\n#     return result\n \n# def generate_and_tokenize_prompt(data_point):\n#     tokenized_full_prompt = generate_prompt(data_point)\n#     return tokenized_full_prompt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.695805Z",
     "iopub.execute_input": "2023-11-28T08:52:39.696090Z",
     "iopub.status.idle": "2023-11-28T08:52:39.705475Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.696067Z",
     "shell.execute_reply": "2023-11-28T08:52:39.704688Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "CUTOFF_LEN = 3584\n\ndef generate_prompt(data_point):\n    promt = f\"\"\"<s>system\n{data_point['system']}</s><s>user\n{data_point['user']}</s><s>bot\n{data_point['bot']}</s>\"\"\"\n    #     print(promt)\n    return promt\n \n    \ndef tokenize (prompt, add_eos_token=True):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN,\n        padding=False,\n        return_tensors=None,\n    )\n    if (\n        result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < CUTOFF_LEN\n        and add_eos_token\n    ):\n        \n        result[\"input_ids\"].append(tokenizer.eos_token_id)\n        result[\"attention_mask\"].append(1)\n        \n        \n    \n    result[\"labels\"] = result[\"input_ids\"].copy()\n\n    return result\n \ndef generate_and_tokenize_prompt(data_point):\n    full_prompt = generate_prompt(data_point)\n    tokenized_full_prompt = tokenize(full_prompt)\n#     print(tokenized_full_prompt)\n    return tokenized_full_prompt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.706525Z",
     "iopub.execute_input": "2023-11-28T08:52:39.706790Z",
     "iopub.status.idle": "2023-11-28T08:52:39.719719Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.706767Z",
     "shell.execute_reply": "2023-11-28T08:52:39.718901Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(generate_and_tokenize_prompt(data[\"train\"][0]))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:39.720681Z",
     "iopub.execute_input": "2023-11-28T08:52:39.721001Z",
     "iopub.status.idle": "2023-11-28T08:52:39.732380Z",
     "shell.execute_reply.started": "2023-11-28T08:52:39.720968Z",
     "shell.execute_reply": "2023-11-28T08:52:39.731596Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [1, 1, 1788, 13, 30041, 29982, 1866, 29942, 753, 642, 29919, 25366, 10539, 989, 933, 606, 20658, 6052, 1866, 23567, 1229, 2, 1, 1404, 13, 30054, 29972, 29942, 753, 717, 1866, 11650, 1868, 23567, 1229, 6495, 20658, 6052, 606, 10539, 989, 933, 29889, 14966, 8160, 11213, 4054, 23761, 490, 21986, 27768, 2494, 16919, 10786, 531, 1186, 2583, 1282, 989, 1840, 606, 5023, 13, 16396, 29988, 1155, 3327, 785, 6408, 662, 587, 11752, 29970, 665, 1578, 693, 3162, 29871, 8089, 25656, 13781, 29892, 490, 20935, 11883, 10689, 576, 642, 733, 29904, 1093, 4715, 570, 16186, 1779, 4394, 16844, 29988, 18992, 3029, 5406, 3325, 3541, 665, 1186, 13174, 1282, 14820, 1447, 11106, 29889, 2081, 2194, 8847, 3378, 1257, 8440, 730, 1892, 17683, 26907, 30002, 6052, 606, 8433, 507, 4962, 1587, 6785, 5752, 29889, 2, 1, 9225, 13, 29961, 10998, 8489, 22099, 30074, 29910, 29988, 1155, 3327, 742, 525, 16553, 2396, 525, 29957, 587, 11752, 29970, 665, 1578, 693, 3162, 29871, 8089, 25656, 13781, 29892, 490, 20935, 11883, 10689, 576, 642, 733, 29904, 1093, 4715, 570, 16186, 1779, 4394, 16844, 29988, 18992, 3029, 5406, 3325, 3541, 665, 1186, 13174, 1282, 14820, 1447, 11106, 6169, 6525, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1788, 13, 30041, 29982, 1866, 29942, 753, 642, 29919, 25366, 10539, 989, 933, 606, 20658, 6052, 1866, 23567, 1229, 2, 1, 1404, 13, 30054, 29972, 29942, 753, 717, 1866, 11650, 1868, 23567, 1229, 6495, 20658, 6052, 606, 10539, 989, 933, 29889, 14966, 8160, 11213, 4054, 23761, 490, 21986, 27768, 2494, 16919, 10786, 531, 1186, 2583, 1282, 989, 1840, 606, 5023, 13, 16396, 29988, 1155, 3327, 785, 6408, 662, 587, 11752, 29970, 665, 1578, 693, 3162, 29871, 8089, 25656, 13781, 29892, 490, 20935, 11883, 10689, 576, 642, 733, 29904, 1093, 4715, 570, 16186, 1779, 4394, 16844, 29988, 18992, 3029, 5406, 3325, 3541, 665, 1186, 13174, 1282, 14820, 1447, 11106, 29889, 2081, 2194, 8847, 3378, 1257, 8440, 730, 1892, 17683, 26907, 30002, 6052, 606, 8433, 507, 4962, 1587, 6785, 5752, 29889, 2, 1, 9225, 13, 29961, 10998, 8489, 22099, 30074, 29910, 29988, 1155, 3327, 742, 525, 16553, 2396, 525, 29957, 587, 11752, 29970, 665, 1578, 693, 3162, 29871, 8089, 25656, 13781, 29892, 490, 20935, 11883, 10689, 576, 642, 733, 29904, 1093, 4715, 570, 16186, 1779, 4394, 16844, 29988, 18992, 3029, 5406, 3325, 3541, 665, 1186, 13174, 1282, 14820, 1447, 11106, 6169, 6525, 2]}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "train_data = (\n    data[\"train\"].map(generate_and_tokenize_prompt)\n)\n\nval_data = (\n    data[\"validation\"].map(generate_and_tokenize_prompt)\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:43.217495Z",
     "iopub.execute_input": "2023-11-28T08:52:43.217886Z",
     "iopub.status.idle": "2023-11-28T08:52:43.361553Z",
     "shell.execute_reply.started": "2023-11-28T08:52:43.217843Z",
     "shell.execute_reply": "2023-11-28T08:52:43.360691Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/22 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5c06f992cd34ddb983c13734a3ed7e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a1474bf8df741f783f2a39c0e501d7a"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Обучаем модель",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:44.777099Z",
     "iopub.execute_input": "2023-11-28T08:52:44.777741Z",
     "iopub.status.idle": "2023-11-28T08:52:44.782151Z",
     "shell.execute_reply.started": "2023-11-28T08:52:44.777710Z",
     "shell.execute_reply": "2023-11-28T08:52:44.781011Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "BATCH_SIZE = 4\nMICRO_BATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nLEARNING_RATE = 3e-4\nTRAIN_STEPS = 100\nOUTPUT_DIR = \"/kaggle/working/tmp\"\n\ntraining_arguments = transformers.TrainingArguments(\n            per_device_train_batch_size=MICRO_BATCH_SIZE,\n            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n#             warmup_steps=200,\n            max_steps=TRAIN_STEPS,\n            learning_rate=LEARNING_RATE,\n            fp16=True,\n            logging_steps=10,\n            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\",\n            save_strategy=\"steps\",\n            eval_steps=10,\n            save_steps=10,\n            output_dir=OUTPUT_DIR,\n            save_total_limit=10,\n            load_best_model_at_end=True,\n            report_to=None,\n            overwrite_output_dir=True, # Overwrite the content of the output dir\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:45.297826Z",
     "iopub.execute_input": "2023-11-28T08:52:45.298631Z",
     "iopub.status.idle": "2023-11-28T08:52:45.309753Z",
     "shell.execute_reply.started": "2023-11-28T08:52:45.298594Z",
     "shell.execute_reply": "2023-11-28T08:52:45.308900Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "text": "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "data_collator = transformers.DataCollatorForSeq2Seq(\n    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:46.487328Z",
     "iopub.execute_input": "2023-11-28T08:52:46.487728Z",
     "iopub.status.idle": "2023-11-28T08:52:46.492712Z",
     "shell.execute_reply.started": "2023-11-28T08:52:46.487696Z",
     "shell.execute_reply": "2023-11-28T08:52:46.491636Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "trainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=training_arguments,\n    data_collator=data_collator\n)\nmodel = torch.compile(model)\ntrainer.train()\nmodel.save_pretrained(OUTPUT_DIR)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:52:48.423685Z",
     "iopub.execute_input": "2023-11-28T08:52:48.424709Z",
     "iopub.status.idle": "2023-11-28T09:00:32.672314Z",
     "shell.execute_reply.started": "2023-11-28T08:52:48.424671Z",
     "shell.execute_reply": "2023-11-28T09:00:32.671429Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 07:37, Epoch 18/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.139700</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.716100</td>\n      <td>0.594529</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.271100</td>\n      <td>0.553778</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.110700</td>\n      <td>0.521688</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.057300</td>\n      <td>0.536120</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.040300</td>\n      <td>0.554408</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.033600</td>\n      <td>0.561207</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.032200</td>\n      <td>0.574507</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.031200</td>\n      <td>0.575170</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.030100</td>\n      <td>0.576941</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Конвертирование модели в gguf",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos._exit(00) # Перезапустить ядро",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%mkdir /kaggle/tmp\n%cd /kaggle/tmp\n%ls",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:00:45.642790Z",
     "iopub.execute_input": "2023-11-28T09:00:45.643566Z",
     "iopub.status.idle": "2023-11-28T09:00:47.507314Z",
     "shell.execute_reply.started": "2023-11-28T09:00:45.643523Z",
     "shell.execute_reply": "2023-11-28T09:00:47.506297Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/tmp\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "self_instruct_dir = '/kaggle/working/rulm/self_instruct'\n\ncheckpoint = \"/kaggle/working/tmp/checkpoint-40\"\n\nmerged_model_name = '/kaggle/tmp/merged_test_model.pt'\n\nself_instruct_dir",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:00:48.016771Z",
     "iopub.execute_input": "2023-11-28T09:00:48.017667Z",
     "iopub.status.idle": "2023-11-28T09:00:48.025184Z",
     "shell.execute_reply.started": "2023-11-28T09:00:48.017631Z",
     "shell.execute_reply": "2023-11-28T09:00:48.024170Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/kaggle/working/rulm/self_instruct'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%cd {self_instruct_dir}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:00:49.738478Z",
     "iopub.execute_input": "2023-11-28T09:00:49.739259Z",
     "iopub.status.idle": "2023-11-28T09:00:49.745387Z",
     "shell.execute_reply.started": "2023-11-28T09:00:49.739227Z",
     "shell.execute_reply": "2023-11-28T09:00:49.744388Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/rulm/self_instruct\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Склеиваем вместе обученные адаптеры с базовой моделью, сохраняем результат в формат PyTorch\n##### Параметры\n1. чекпоинт лучшей эпохи\n2. куда сохраняем\n3. Используем cpu или cuda\n4. Выгружаем ли половину тензеров в оперативку",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!python -m src.tools.convert_to_native {checkpoint} {merged_model_name} --device=cuda --enable_offloading\n# assert (output_dir / merged_model_name).exists()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:00:51.568608Z",
     "iopub.execute_input": "2023-11-28T09:00:51.568963Z",
     "iopub.status.idle": "2023-11-28T09:03:44.278205Z",
     "shell.execute_reply.started": "2023-11-28T09:00:51.568929Z",
     "shell.execute_reply": "2023-11-28T09:03:44.276996Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:58<00:00, 29.00s/it]\n100%|█████████████████████████████████████████| 291/291 [00:05<00:00, 52.85it/s]\nSaving state_dict...\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/tmp/\n%ls",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:03:44.281993Z",
     "iopub.execute_input": "2023-11-28T09:03:44.282286Z",
     "iopub.status.idle": "2023-11-28T09:03:45.272288Z",
     "shell.execute_reply.started": "2023-11-28T09:03:44.282259Z",
     "shell.execute_reply": "2023-11-28T09:03:45.271295Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/tmp\nmerged_test_model.pt\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Конвертируем склеенную модель в 16-битный формат GGUF (llama.cpp)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos._exit(00) # Перезапустить ядро",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:03:45.273694Z",
     "iopub.execute_input": "2023-11-28T09:03:45.274022Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga2_7b_lora\", use_fast=False)\ntokenizer.save_pretrained('/kaggle/working/tmp/checkpoint-40/')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:05:32.176147Z",
     "iopub.execute_input": "2023-11-28T09:05:32.176489Z",
     "iopub.status.idle": "2023-11-28T09:05:32.442154Z",
     "shell.execute_reply.started": "2023-11-28T09:05:32.176452Z",
     "shell.execute_reply": "2023-11-28T09:05:32.441344Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
     "output_type": "stream"
    },
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "('/kaggle/working/tmp/checkpoint-40/tokenizer_config.json',\n '/kaggle/working/tmp/checkpoint-40/special_tokens_map.json',\n '/kaggle/working/tmp/checkpoint-40/tokenizer.model',\n '/kaggle/working/tmp/checkpoint-40/added_tokens.json')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/working/llama.cpp",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:05:32.443222Z",
     "iopub.execute_input": "2023-11-28T09:05:32.443509Z",
     "iopub.status.idle": "2023-11-28T09:05:34.318112Z",
     "shell.execute_reply.started": "2023-11-28T09:05:32.443477Z",
     "shell.execute_reply": "2023-11-28T09:05:34.317034Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/llama.cpp\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%%capture\n!python3 -m pip install -r requirements.txt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:05:34.319495Z",
     "iopub.execute_input": "2023-11-28T09:05:34.319778Z",
     "iopub.status.idle": "2023-11-28T09:05:49.316046Z",
     "shell.execute_reply.started": "2023-11-28T09:05:34.319754Z",
     "shell.execute_reply": "2023-11-28T09:05:49.314745Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model_dir = '/kaggle/tmp/merged_test_model.pt'\ncheckpoint = \"/kaggle/working/tmp/checkpoint-40\"\noutput_model = \"/kaggle/tmp/model-f16.gguf\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:05:49.318060Z",
     "iopub.execute_input": "2023-11-28T09:05:49.318441Z",
     "iopub.status.idle": "2023-11-28T09:05:49.323348Z",
     "shell.execute_reply.started": "2023-11-28T09:05:49.318403Z",
     "shell.execute_reply": "2023-11-28T09:05:49.322357Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python convert.py {model_dir} --vocab-dir {checkpoint} --outfile {output_model} --outtype f16 --ctx 4096",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:05:49.324844Z",
     "iopub.execute_input": "2023-11-28T09:05:49.325204Z",
     "iopub.status.idle": "2023-11-28T09:07:29.088519Z",
     "shell.execute_reply.started": "2023-11-28T09:05:49.325172Z",
     "shell.execute_reply": "2023-11-28T09:07:29.087494Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading model file /kaggle/tmp/merged_test_model.pt\nparams = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/kaggle/tmp'))\nLoading vocab file '/kaggle/working/tmp/checkpoint-40/tokenizer.model', type 'spm'\ntok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 4096]\nlayers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\nlayers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\nlayers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\nlayers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\nlayers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [4096]\nlayers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [4096]\nlayers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\nlayers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\nlayers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\nlayers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\nlayers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [4096]\nlayers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [4096]\nlayers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\nlayers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\nlayers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\nlayers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\nlayers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [4096]\nlayers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [4096]\nlayers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\nlayers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\nlayers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\nlayers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\nlayers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [4096]\nlayers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [4096]\nlayers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\nlayers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\nlayers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\nlayers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\nlayers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [4096]\nlayers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [4096]\nlayers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\nlayers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\nlayers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\nlayers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\nlayers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [4096]\nlayers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [4096]\nlayers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\nlayers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\nlayers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\nlayers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\nlayers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [4096]\nlayers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [4096]\nlayers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\nlayers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\nlayers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\nlayers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\nlayers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [4096]\nlayers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [4096]\nlayers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\nlayers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\nlayers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\nlayers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\nlayers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [4096]\nlayers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [4096]\nlayers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\nlayers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\nlayers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\nlayers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\nlayers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\nlayers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\nlayers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\nlayers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [4096]\nlayers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [4096]\nlayers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\nlayers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\nlayers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\nlayers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [4096, 4096]\nlayers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [4096]\nlayers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [4096]\nlayers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\nlayers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\nlayers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\nlayers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [4096, 4096]\nlayers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [4096]\nlayers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [4096]\nlayers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\nlayers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\nlayers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\nlayers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [4096, 4096]\nlayers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [4096]\nlayers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [4096]\nlayers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\nlayers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\nlayers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\nlayers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [4096, 4096]\nlayers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [4096]\nlayers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [4096]\nlayers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\nlayers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\nlayers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\nlayers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [4096, 4096]\nlayers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [4096]\nlayers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [4096]\nlayers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\nlayers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\nlayers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\nlayers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [4096, 4096]\nlayers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [4096]\nlayers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [4096]\nlayers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\nlayers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\nlayers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\nlayers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [4096, 4096]\nlayers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [4096]\nlayers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [4096]\nlayers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\nlayers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\nlayers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\nlayers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [4096, 4096]\nlayers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [4096]\nlayers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [4096]\nlayers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\nlayers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\nlayers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\nlayers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [4096, 4096]\nlayers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [4096]\nlayers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [4096]\nlayers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\nlayers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\nlayers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\nlayers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [4096, 4096]\nlayers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [4096]\nlayers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [4096]\nlayers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\nlayers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\nlayers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\nlayers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [4096, 4096]\nlayers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [4096]\nlayers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [4096]\nlayers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\nlayers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\nlayers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\nlayers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [4096, 4096]\nlayers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [4096]\nlayers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [4096]\nlayers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\nlayers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\nlayers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\nlayers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [4096, 4096]\nlayers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [4096]\nlayers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [4096]\nlayers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\nlayers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\nlayers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\nlayers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [4096, 4096]\nlayers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [4096]\nlayers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [4096]\nlayers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\nlayers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\nlayers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\nlayers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [4096, 4096]\nlayers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [4096]\nlayers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [4096]\nlayers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\nlayers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\nlayers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\nlayers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [4096, 4096]\nlayers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [4096]\nlayers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [4096]\nlayers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\nlayers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\nlayers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\nlayers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [4096, 4096]\nlayers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [4096]\nlayers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [4096]\nlayers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\nlayers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\nlayers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\nlayers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [4096, 4096]\nlayers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [4096]\nlayers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [4096]\nlayers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\nlayers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\nlayers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\nlayers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [4096, 4096]\nlayers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [4096]\nlayers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [4096]\nlayers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\nlayers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\nlayers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\nlayers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [4096, 4096]\nlayers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [4096]\nlayers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [4096]\nlayers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\nlayers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\nlayers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\nlayers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [4096, 4096]\nlayers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [4096]\nlayers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [4096]\nlayers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\nlayers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\nlayers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\nlayers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [4096, 4096]\nlayers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\nlayers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\nlayers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\nlayers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [4096]\nlayers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [4096]\nnorm.weight                                      -> output_norm.weight                       | F16    | [4096]\noutput.weight                                    -> output.weight                            | F16    | [32000, 4096]\nWriting /kaggle/tmp/model-f16.gguf, format 1\nIgnoring added_tokens.json since model matches vocab size without it.\ngguf: This GGUF file is for Little Endian only\n[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   1\n[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   2\n[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   3\n[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   4\n[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   4\n[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   5\n[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   5\n[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   5\n[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   6\n[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   6\n[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   6\n[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   6\n[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   7\n[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   7\n[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   7\n[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7\n[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   7\n[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7\n[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9\n[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   9\n[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   9\n[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   9\n[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   9\n[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9\n[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   9\n[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   9\n[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10\n[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10\n[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  11\n[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  11\n[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  11\n[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11\n[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  11\n[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  11\n[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  11\n[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  12\n[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  13\n[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  14\n[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  14\n[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  14\n[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  14\n[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  14\n[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  14\n[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  14\n[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  16\n[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  16\n[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  16\n[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  16\n[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  16\n[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  16\n[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  16\n[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  17\n[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  17\n[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  18\n[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  19\n[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  19\n[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  19\n[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  19\n[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  19\n[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  19\n[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  20\n[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  20\n[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  20\n[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  21\n[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  21\n[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  21\n[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  21\n[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  22\n[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  22\n[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  22\n[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  22\n[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  22\n[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  22\n[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  22\n[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  23\n[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  23\n[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  24\n[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  24\n[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  24\n[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  24\n[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  24\n[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  24\n[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  24\n[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  24\n[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  25\n[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  25\n[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  25\n[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  26\n[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  27\n[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  27\n[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  27\n[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  27\n[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  27\n[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  27\n[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  27\n[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  28\n[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  29\n[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  30\n[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  30\n[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  30\n[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  30\n[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  30\n[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  30\n[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  31\n[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  31\n[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  32\n[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  32\n[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  33\n[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  33\n[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  33\n[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  33\n[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  34\n[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  34\n[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  34\n[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  35\n[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  36\n[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  36\n[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  36\n[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  36\n[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  36\n[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  36\n[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  37\n[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  37\n[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  38\n[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  38\n[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  38\n[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  38\n[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  38\n[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  39\n[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  39\n[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  39\n[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  39\n[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  40\n[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  40\n[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  40\n[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  40\n[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  41\n[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  41\n[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  41\n[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  42\n[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  42\n[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  43\n[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  43\n[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  43\n[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  43\n[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  43\n[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  44\n[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  44\n[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  44\n[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  45\n[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  46\n[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  46\n[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  46\n[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  46\n[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  46\n[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  47\n[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  47\n[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  48\n[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  48\n[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  48\n[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  48\n[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  48\n[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  48\n[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  48\n[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  49\n[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  49\n[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  50\n[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  50\n[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  51\n[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  51\n[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  51\n[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  51\n[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  52\n[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  52\n[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  52\n[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  53\n[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  53\n[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  54\n[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  54\n[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  54\n[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  54\n[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  54\n[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  54\n[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  55\n[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  56\n[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  57\n[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  57\n[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  57\n[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  57\n[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  57\n[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  57\n[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  58\n[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  58\n[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  58\n[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  59\n[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  59\n[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  59\n[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  59\n[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  59\n[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  59\n[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  59\n[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  60\n[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  61\n[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  62\n[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  62\n[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  62\n[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  62\n[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  62\n[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  62\n[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  63\n[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  63\n[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  64\n[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  65\n[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  65\n[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  65\n[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  65\n[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  65\n[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  66\n[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  66\n[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  67\n[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  68\n[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  68\n[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  68\n[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  68\n[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  68\n[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  68\n[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  68\n[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  69\n[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  69\n[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  70\n[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  70\n[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  70\n[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  70\n[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  71\n[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  71\n[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  71\n[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  72\n[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  72\n[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  73\n[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  73\n[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  73\n[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  73\n[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  73\n[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  73\n[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  74\n[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  75\n[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  76\n[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  76\n[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  76\n[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  76\n[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  76\n[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  76\n[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  77\n[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  77\n[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  78\n[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  78\n[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  78\n[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  78\n[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  78\nWrote /kaggle/tmp/model-f16.gguf\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/tmp/\n%ls",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:07:29.089961Z",
     "iopub.execute_input": "2023-11-28T09:07:29.090266Z",
     "iopub.status.idle": "2023-11-28T09:07:30.051913Z",
     "shell.execute_reply.started": "2023-11-28T09:07:29.090236Z",
     "shell.execute_reply": "2023-11-28T09:07:30.050527Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/tmp\nmerged_test_model.pt  model-f16.gguf\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Квантуем моедль в 4 бита и 8 бит",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos._exit(00) # Перезапустить ядро",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:07:30.056180Z",
     "iopub.execute_input": "2023-11-28T09:07:30.056496Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/working/llama.cpp",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:07:45.078590Z",
     "iopub.execute_input": "2023-11-28T09:07:45.079491Z",
     "iopub.status.idle": "2023-11-28T09:07:45.090849Z",
     "shell.execute_reply.started": "2023-11-28T09:07:45.079455Z",
     "shell.execute_reply": "2023-11-28T09:07:45.089834Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/llama.cpp\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!make quantize",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:07:45.612556Z",
     "iopub.execute_input": "2023-11-28T09:07:45.613047Z",
     "iopub.status.idle": "2023-11-28T09:08:14.715631Z",
     "shell.execute_reply.started": "2023-11-28T09:07:45.613011Z",
     "shell.execute_reply": "2023-11-28T09:08:14.714414Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "I llama.cpp build info: \nI UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native \nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \nI NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \"\nI LDFLAGS:    \nI CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nI CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/build-info.cpp -o build-info.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml.c -o ggml.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c llama.cpp -o llama.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-alloc.c -o ggml-alloc.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-backend.c -o ggml-backend.o\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native     -c ggml-quants.c -o ggml-quants.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model_gguf = \"/kaggle/tmp/model-f16.gguf\"\nquant_model = \"/kaggle/working/model-q4_0.gguf\"\nquantization_type = \"q4_0\" #@param [\"q4_0\", \"q4_1\"] {allow-input: true}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:08:14.720138Z",
     "iopub.execute_input": "2023-11-28T09:08:14.720541Z",
     "iopub.status.idle": "2023-11-28T09:08:14.725613Z",
     "shell.execute_reply.started": "2023-11-28T09:08:14.720503Z",
     "shell.execute_reply": "2023-11-28T09:08:14.724599Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "! ./quantize {model_gguf} {quant_model} {quantization_type}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:08:14.726873Z",
     "iopub.execute_input": "2023-11-28T09:08:14.727188Z",
     "iopub.status.idle": "2023-11-28T09:09:33.064847Z",
     "shell.execute_reply.started": "2023-11-28T09:08:14.727158Z",
     "shell.execute_reply": "2023-11-28T09:09:33.063540Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "main: build = 1574 (8406b09)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing '/kaggle/tmp/model-f16.gguf' to '/kaggle/working/model-q4_0.gguf' as Q4_0\nllama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from /kaggle/tmp/model-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  223:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  241:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  259:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  277:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  290:                    output.weight f16      [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllama_model_quantize_internal: meta size = 740928 bytes\n[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.034 0.008 0.012 0.019 0.031 0.051 0.084 0.149 0.251 0.150 0.085 0.051 0.031 0.019 0.012 0.010 \n[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.034 0.008 0.013 0.021 0.034 0.056 0.090 0.149 0.214 0.150 0.091 0.056 0.035 0.022 0.013 0.011 \n[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.036 0.053 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.024 0.020 \n[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.011 0.017 0.028 0.044 0.068 0.100 0.135 0.154 0.135 0.100 0.068 0.044 0.028 0.017 0.014 \n[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[   7/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[   8/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.034 0.052 0.074 0.098 0.121 0.132 0.121 0.098 0.074 0.052 0.034 0.022 0.018 \n[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.034 0.052 0.074 0.099 0.121 0.131 0.121 0.099 0.074 0.052 0.034 0.022 0.018 \n[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.014 0.023 0.035 0.052 0.074 0.097 0.119 0.130 0.119 0.097 0.074 0.052 0.035 0.023 0.019 \n[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.031 0.047 0.070 0.099 0.129 0.146 0.129 0.099 0.070 0.047 0.031 0.020 0.016 \n[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  16/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  17/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.120 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  25/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  26/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  34/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  35/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  43/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  44/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  52/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  53/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  61/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  62/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  70/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  71/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  79/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  80/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  88/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  89/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  97/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  98/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 106/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 107/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 115/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 116/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 124/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 125/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 133/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 134/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 142/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 143/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 151/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 152/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 160/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 161/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 169/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 170/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 178/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 179/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 187/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 188/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 196/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 197/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.057 0.039 0.025 0.021 \n[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 205/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 206/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 214/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 215/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 223/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 224/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 232/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 233/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 241/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 242/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 250/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 251/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 259/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 260/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 268/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 269/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 277/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 278/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 286/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 287/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.023 0.036 0.054 0.075 0.098 0.116 0.124 0.116 0.098 0.075 0.054 0.036 0.023 0.019 \n[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \nllama_model_quantize_internal: model size  = 12853.02 MB\nllama_model_quantize_internal: quant size  =  3647.87 MB\nllama_model_quantize_internal: hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n\nmain: quantize time = 77368.15 ms\nmain:    total time = 77368.15 ms\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Проверка gguf модели",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%cd ~\n!git clone --recursive https://github.com/ggerganov/llama.cpp.git\n%cd llama.cpp\n!make LLAMA_CUBLAS=1 -j libllama.so\n\n# HACK: Use custom compiled libllama.so\n%cp ~/llama.cpp/libllama.so /opt/conda/lib/python3.10/site-packages/llama_cpp/libllama.so",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:09:41.010901Z",
     "iopub.execute_input": "2023-11-28T09:09:41.011318Z",
     "iopub.status.idle": "2023-11-28T09:10:12.768383Z",
     "shell.execute_reply.started": "2023-11-28T09:09:41.011285Z",
     "shell.execute_reply": "2023-11-28T09:10:12.767049Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "/root\nCloning into 'llama.cpp'...\nremote: Enumerating objects: 12546, done.\u001B[K\nremote: Counting objects: 100% (5693/5693), done.\u001B[K\nremote: Compressing objects: 100% (472/472), done.\u001B[K\nremote: Total 12546 (delta 5487), reused 5231 (delta 5221), pack-reused 6853\u001B[K\nReceiving objects: 100% (12546/12546), 14.92 MiB | 2.01 MiB/s, done.\nResolving deltas: 100% (8755/8755), done.\n/root/llama.cpp\nI llama.cpp build info: \nI UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native \nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \"\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \nI CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nI CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c llama.cpp -o llama.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml.c -o ggml.o\nnvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \" -c ggml-cuda.cu -o ggml-cuda.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-alloc.c -o ggml-alloc.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-backend.c -o ggml-backend.o\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native     -c ggml-quants.c -o ggml-quants.o\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -shared -fPIC -o libllama.so llama.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# import fire\nfrom llama_cpp import Llama\nfrom tqdm import tqdm",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:10:12.770426Z",
     "iopub.execute_input": "2023-11-28T09:10:12.770761Z",
     "iopub.status.idle": "2023-11-28T09:10:13.128376Z",
     "shell.execute_reply.started": "2023-11-28T09:10:12.770731Z",
     "shell.execute_reply": "2023-11-28T09:10:13.127416Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "SYSTEM_PROMPT = \"Ты извлекаешь термины и определения из текста\"\nSYSTEM_TOKEN = 1788\nUSER_TOKEN = 1404\nBOT_TOKEN = 9225\nLINEBREAK_TOKEN = 13\n\ntop_k=40\ntop_p=0.5\ntemperature=0.01\nrepeat_penalty=1.1\n\n\nROLE_TOKENS = {\n    \"user\": USER_TOKEN,\n    \"bot\": BOT_TOKEN,\n    \"system\": SYSTEM_TOKEN\n}\n\n\ndef get_message_tokens(model, role, content):\n    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n    message_tokens.insert(1, ROLE_TOKENS[role])\n    message_tokens.insert(2, LINEBREAK_TOKEN)\n    message_tokens.append(model.token_eos())\n    return message_tokens\n\n\ndef get_system_tokens(model):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    }\n    return get_message_tokens(model, **system_message)\n\ndef chat_saiga(message, model):\n    system_tokens = get_system_tokens(model)\n    tokens = system_tokens\n    # model.eval(tokens)\n    \n    message_tokens = get_message_tokens(model=model, role=\"user\", content=message)\n    role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n    tokens += message_tokens + role_tokens\n    # print(tokens)\n    # detokenize = model.detokenize(tokens)\n    # print(model.tokenize(full_prompt))\n    generator = model.generate(\n        tokens,\n        top_k = top_k,\n        top_p = top_p,\n        temp = temperature,\n        repeat_penalty = repeat_penalty,\n        reset = True\n    )\n    # print(len([token for token in generator]))\n    \n    result_list = []\n    \n    for token in generator:\n        token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n        tokens.append(token)\n        if token == model.token_eos():\n            break\n        print(token_str, end=\"\", flush=True)\n        result_list.append(token_str)\n    return ''.join(result_list)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:11:30.564333Z",
     "iopub.execute_input": "2023-11-28T09:11:30.565476Z",
     "iopub.status.idle": "2023-11-28T09:11:30.577177Z",
     "shell.execute_reply.started": "2023-11-28T09:11:30.565415Z",
     "shell.execute_reply": "2023-11-28T09:11:30.576233Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%cd /kaggle/tmp\n%ls",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:10:13.143715Z",
     "iopub.execute_input": "2023-11-28T09:10:13.144317Z",
     "iopub.status.idle": "2023-11-28T09:10:14.102115Z",
     "shell.execute_reply.started": "2023-11-28T09:10:13.144287Z",
     "shell.execute_reply": "2023-11-28T09:10:14.101042Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/tmp\nmerged_test_model.pt  model-f16.gguf\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%%capture\ntry:\n    del model\nexcept:\n    pass\n\n# model_path = '/kaggle/working/model-q4_0.gguf'\nmodel_path = '/kaggle/working/model-q4_0.gguf'\nn_ctx = 3096 #\n\nmodel = Llama(\n        model_path = model_path,\n        n_ctx = n_ctx,\n        n_gpu_layers=-1\n) # n_gpu_layers=-1  параметр для переноса модели на GPU",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:10:14.103527Z",
     "iopub.execute_input": "2023-11-28T09:10:14.103876Z",
     "iopub.status.idle": "2023-11-28T09:10:21.388246Z",
     "shell.execute_reply.started": "2023-11-28T09:10:14.103832Z",
     "shell.execute_reply": "2023-11-28T09:10:21.387077Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0\nllama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /kaggle/working/model-q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  223:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  259:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  277:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = mostly Q4_0\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =   70.42 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 35/35 layers to GPU\nllm_load_tensors: VRAM used: 3577.55 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 3096\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading k cache to GPU\nllama_kv_cache_init: VRAM kv self = 1548.00 MiB\nllama_new_context_with_model: kv self size  = 1548.00 MiB\nllama_build_graph: non-view tensors processed: 740/740\nllama_new_context_with_model: compute buffer total size = 226.61 MiB\nllama_new_context_with_model: VRAM scratch buffer: 223.55 MiB\nllama_new_context_with_model: total VRAM used: 5349.11 MiB (model: 3577.55 MiB, context: 1771.55 MiB)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "%%time\npromt = \"\"\"\nИзвлеки из данного текста все определения и термины.\nВывод должен быть в виде списка словарей с ключами term и definition\n\nМашинное обучение (ML) — это использование математических моделей данных, которые помогают компьютеру обучаться без непосредственных инструкций.\nОно считается одной из форм искусственного интеллекта (ИИ).\n\"\"\"\nchat_saiga(promt, model)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T09:15:01.539642Z",
     "iopub.execute_input": "2023-11-28T09:15:01.540154Z",
     "iopub.status.idle": "2023-11-28T09:15:04.416775Z",
     "shell.execute_reply.started": "2023-11-28T09:15:01.540120Z",
     "shell.execute_reply": "2023-11-28T09:15:04.415800Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "[",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Llama.generate: prefix-match hit\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "{'term':'Машинное обучение (ML)', 'definition': 'использование математических моделей данных, которые помогают компьютеру обучаться без непосредственных инструкций.'}]CPU times: user 2.86 s, sys: 47.7 ms, total: 2.91 s\nWall time: 2.87 s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"[{'term':'Машинное обучение (ML)', 'definition': 'использование математических моделей данных, которые помогают компьютеру обучаться без непосредственных инструкций.'}]\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Проверяем модель на адекватность",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\nDEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\nDEFAULT_SYSTEM_PROMPT = \"Ты извлекаешь термины и определения из текста\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass Conversation:\n    def __init__(\n        self,\n        message_template = DEFAULT_MESSAGE_TEMPLATE,\n        system_prompt = DEFAULT_SYSTEM_PROMPT,\n        response_template = DEFAULT_RESPONSE_TEMPLATE\n    ):\n        self.message_template = message_template\n        self.response_template = response_template\n        self.messages = [{\n            \"role\": \"system\",\n            \"content\": system_prompt\n        }]\n\n    def add_user_message(self, message):\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": message\n        })\n\n    def add_bot_message(self, message):\n        self.messages.append({\n            \"role\": \"bot\",\n            \"content\": message\n        })\n\n    def get_prompt(self, tokenizer):\n        final_text = \"\"\n        for message in self.messages:\n            message_text = self.message_template.format(**message)\n            final_text += message_text\n        final_text += DEFAULT_RESPONSE_TEMPLATE\n        return final_text.strip()\n\n\n\ndef generate(model, tokenizer, prompt, generation_config):\n    data = tokenizer(prompt,\n                     return_tensors=\"pt\",\n                     add_special_tokens=False,\n#                      padding=True,\n#                     truncation=True\n                    )\n    data = {k: v.to(device) for k, v in data.items()}\n    \n    output_ids = model.generate(\n        **data,\n        generation_config = generation_config,\n#         remove_invalid_values = True\n    )[0]\n    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return output.strip()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-22T08:59:09.587251Z",
     "iopub.execute_input": "2023-11-22T08:59:09.587683Z",
     "iopub.status.idle": "2023-11-22T08:59:09.602609Z",
     "shell.execute_reply.started": "2023-11-22T08:59:09.587640Z",
     "shell.execute_reply": "2023-11-22T08:59:09.601588Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\ndef get_message(inputs):\n    result = []\n    for inp in inputs:\n        conversation = Conversation()\n        conversation.add_user_message(inp)\n        prompt = conversation.get_prompt(tokenizer)\n        print(prompt)\n        output = generate(model, tokenizer, prompt, generation_config)\n        result.append(output)\n    return result",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-22T08:59:10.697269Z",
     "iopub.execute_input": "2023-11-22T08:59:10.698122Z",
     "iopub.status.idle": "2023-11-22T08:59:10.704344Z",
     "shell.execute_reply.started": "2023-11-22T08:59:10.698078Z",
     "shell.execute_reply": "2023-11-22T08:59:10.703315Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "p = \"\"\"\nИзвлеки из данного текста все определения и термины.\nВывод должен быть в виде списка словарей с ключами term и definition\n\nЛинейная модель — модель, отображающая состояние или функционирование системы таким образом, что все взаимозависимости в ней принимаются линейными (см. Линейная зависимость, Линейность в экономике). Соответственно, она может формулироваться в виде одного линейного уравнения или системы линейных уравнений.\n\"\"\"\nget_message([p])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-22T08:59:11.958724Z",
     "iopub.execute_input": "2023-11-22T08:59:11.959131Z",
     "iopub.status.idle": "2023-11-22T08:59:34.388707Z",
     "shell.execute_reply.started": "2023-11-22T08:59:11.959081Z",
     "shell.execute_reply": "2023-11-22T08:59:34.387648Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import notebook_login\n \nnotebook_login()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-22T11:09:41.030450Z",
     "iopub.execute_input": "2023-11-22T11:09:41.031620Z",
     "iopub.status.idle": "2023-11-22T11:09:41.399126Z",
     "shell.execute_reply.started": "2023-11-22T11:09:41.031579Z",
     "shell.execute_reply": "2023-11-22T11:09:41.398169Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model.push_to_hub(\"YarKo69/Saiga_2_7b_fine_tune_custom_data\", use_auth_token=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-22T11:09:55.808607Z",
     "iopub.execute_input": "2023-11-22T11:09:55.809439Z",
     "iopub.status.idle": "2023-11-22T11:09:56.192177Z",
     "shell.execute_reply.started": "2023-11-22T11:09:55.809398Z",
     "shell.execute_reply": "2023-11-22T11:09:56.190934Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "{\"system\": \"Ты извлекаешь термины и определения из текста\", \"user\": \"Извлеки из данного текста все определения и термины. Вывод должен быть в виде списка словарей с ключами term и definition\\n Суффикс - это значимая часть слова, которая находится после корня и перед окончанием, служит для образования новых слов.\", \"bot\": \"[{'term':'Суффикс', 'definition': 'значимая часть слова, которая находится после корня и перед окончанием, служит для образования новых слов.'}]\"}\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T10:43:05.648665Z",
     "iopub.execute_input": "2023-11-21T10:43:05.648964Z",
     "iopub.status.idle": "2023-11-21T10:43:05.656003Z",
     "shell.execute_reply.started": "2023-11-21T10:43:05.648938Z",
     "shell.execute_reply": "2023-11-21T10:43:05.655093Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Гайды по обучению\n* [Омега полезный](https://www.mlexpert.io/machine-learning/tutorials/alpaca-fine-tuning)\n* [Тоже полезный](https://brev.dev/blog/fine-tuning-llama-2-your-own-data#1-load-dataset)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
