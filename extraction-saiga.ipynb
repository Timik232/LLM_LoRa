{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# !pip install accelerate==0.21.0 \\\n#   bitsandbytes==0.40.2 \\\n#   peft==0.5.0 \\\n#   transformers==4.34.0 \\\n#   sentencepiece \\\n#     langchain \\\n#      faiss-cpu \\\n#     sentence-transformers\n\n# !pip install -U bitsandbytes \n# !pip install -U transformers \n# !pip install -U peft ","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:50:19.935438Z","iopub.execute_input":"2023-11-20T16:50:19.936411Z","iopub.status.idle":"2023-11-20T16:51:28.211599Z","shell.execute_reply.started":"2023-11-20T16:50:19.936373Z","shell.execute_reply":"2023-11-20T16:51:28.210448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install \"numpy>=1.12.1\"\n!pip install \"scipy>=1.0.1\"\n!pip install \"torch>=2.0.0\"\n# !pip install transformers==4.34.0\n!pip install transformers==4.35.2\n!pip install accelerate==0.23.0\n!pip install bitsandbytes==0.41.0\n!pip install peft==0.5.0\n!pip install pillow==10.0.1\n# !pip install llama-cpp-python\n# !pip install datasets\n!pip install zstandard\n!pip install jsonlines\n!pip install sentencepiece\n# !pip install fire\n!pip install datasketch==1.5.9\n!pip install nltk==3.8.1\n# !pip install scikit-learn==1.3.0\n!pip install pytest","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:47:36.408378Z","iopub.execute_input":"2023-11-23T07:47:36.408646Z","iopub.status.idle":"2023-11-23T07:50:38.097963Z","shell.execute_reply.started":"2023-11-23T07:47:36.408620Z","shell.execute_reply":"2023-11-23T07:50:38.096773Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:50:38.100355Z","iopub.execute_input":"2023-11-23T07:50:38.101042Z","iopub.status.idle":"2023-11-23T07:50:38.150740Z","shell.execute_reply.started":"2023-11-23T07:50:38.101005Z","shell.execute_reply":"2023-11-23T07:50:38.149816Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:50:38.151901Z","iopub.execute_input":"2023-11-23T07:50:38.152202Z","iopub.status.idle":"2023-11-23T07:50:41.015160Z","shell.execute_reply.started":"2023-11-23T07:50:38.152178Z","shell.execute_reply":"2023-11-23T07:50:41.014252Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"4.35.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n# from langchain.document_loaders import DataFrameLoader\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n# from langchain.embeddings import HuggingFaceEmbeddings\n# from langchain.vectorstores import FAISS\nimport pandas as pd\nfrom peft import PeftModel, PeftConfig\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nimport torch.nn.functional as F\n\nfrom typing import Any, List, Mapping, Optional\n\n# from langchain.callbacks.manager import CallbackManagerForLLMRun\n# from langchain.llms.base import LLM","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:54:30.239269Z","iopub.execute_input":"2023-11-23T07:54:30.239963Z","iopub.status.idle":"2023-11-23T07:54:34.315689Z","shell.execute_reply.started":"2023-11-23T07:54:30.239927Z","shell.execute_reply":"2023-11-23T07:54:34.314804Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- [PEFT код](https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py#L264)\n- [Модель Mistral_saiga_7b ток в точность fp16](https://huggingface.co/Gaivoronsky/Mistral-7B-Saiga)","metadata":{}},{"cell_type":"code","source":"\nfor d in {1:2, 3:4}.items():\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:54:34.317062Z","iopub.execute_input":"2023-11-23T07:54:34.317510Z","iopub.status.idle":"2023-11-23T07:54:34.322800Z","shell.execute_reply.started":"2023-11-23T07:54:34.317481Z","shell.execute_reply":"2023-11-23T07:54:34.321860Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL_NAME = \"IlyaGusev/saiga_mistral_7b_lora\"\n# MODEL_NAME = \"Gaivoronsky/Mistral-7B-Saiga\"\nMODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\nDEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\nDEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n# DEFAULT_SYSTEM_PROMPT = \"Ты извлекаешь информацию из текста\"\nDEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Conversation:\n    def __init__(\n        self,\n        message_template=DEFAULT_MESSAGE_TEMPLATE,\n        system_prompt=DEFAULT_SYSTEM_PROMPT,\n        start_token_id=1,\n        bot_token_id=9225\n    ):\n        self.message_template = message_template\n        self.start_token_id = start_token_id\n        self.bot_token_id = bot_token_id\n        self.messages = [{\n            \"role\": \"system\",\n            \"content\": system_prompt\n        }]\n\n    def get_start_token_id(self):\n        return self.start_token_id\n\n    def get_bot_token_id(self):\n        return self.bot_token_id\n\n    def add_user_message(self, message):\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": message\n        })\n\n    def add_bot_message(self, message):\n        self.messages.append({\n            \"role\": \"bot\",\n            \"content\": message\n        })\n\n    def get_prompt(self, tokenizer):\n        final_text = \"\"\n        for message in self.messages:\n            message_text = self.message_template.format(**message)\n            final_text += message_text\n        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n        return final_text.strip()\n\n\n\ndef generate(model, tokenizer, prompt, generation_config):\n    print()\n    data = tokenizer(prompt,\n                     return_tensors=\"pt\",\n                     add_special_tokens=False,\n    #                      padding=True,\n    #                     truncation=True\n                    )\n    #print(data)\n    data = {k: v.to(device) for k, v in data.items()}\n    \n    output_ids = model.generate(\n        **data,\n        generation_config = generation_config\n    #         remove_invalid_values = True\n    )[0]\n    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n    output = tokenizer.decode(output_ids, skip_special_tokens=False)\n    return output.strip()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T08:03:11.189390Z","iopub.execute_input":"2023-11-23T08:03:11.190361Z","iopub.status.idle":"2023-11-23T08:03:11.202571Z","shell.execute_reply.started":"2023-11-23T08:03:11.190307Z","shell.execute_reply":"2023-11-23T08:03:11.201494Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:54:40.219715Z","iopub.execute_input":"2023-11-23T07:54:40.220386Z","iopub.status.idle":"2023-11-23T07:54:40.226441Z","shell.execute_reply.started":"2023-11-23T07:54:40.220343Z","shell.execute_reply":"2023-11-23T07:54:40.225365Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import time\n\nst_time = time.time()\nconfig = PeftConfig.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    load_in_8bit = True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(\n    model,\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    is_trainable = True,\n    device_map=\"auto\"\n)\n\n\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\nprint(generation_config)\nprint(f'Прошло времени {time.time() - st_time}')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:54:41.479942Z","iopub.execute_input":"2023-11-23T07:54:41.480815Z","iopub.status.idle":"2023-11-23T07:56:35.989624Z","shell.execute_reply.started":"2023-11-23T07:54:41.480780Z","shell.execute_reply":"2023-11-23T07:56:35.988728Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading adapter_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0645fea485419aa0eb63b85bf69513"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31ad42bebd742cbad170bbaf476bb6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da7e112c5c0b4cf3893cfdccfcd1c175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b153eeb0e6440c6ace16f218403d32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3b1c25720c74b7f9bb0e780834f8596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc354d41841460f9b74c5b47a2e8aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00aba92bb09c4e0b82b4d446b8ace239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d335790ede9f4e62990b573136d47209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ff73357a0646a6ad473d2c3f8eb632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/278 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9ec85222a0404d8e6a3c3ba55ed26f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a660ae4c8be4a29b2d87fe52eb02165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ba2da608bb4c31b9e65dd2900f4479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/118 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df739862d82e432c87c06e1b78ecd1c5"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b1c24bb29f4a198df431915647b9d7"}},"metadata":{}},{"name":"stdout","text":"GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_new_tokens\": 3584,\n  \"no_repeat_ngram_size\": 15,\n  \"pad_token_id\": 0,\n  \"repetition_penalty\": 1.2,\n  \"temperature\": 0.5,\n  \"top_k\": 30,\n  \"top_p\": 0.9\n}\n\nПрошло времени 114.5029125213623\n","output_type":"stream"}]},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:35.991213Z","iopub.execute_input":"2023-11-23T07:56:35.991510Z","iopub.status.idle":"2023-11-23T07:56:35.997844Z","shell.execute_reply.started":"2023-11-23T07:56:35.991485Z","shell.execute_reply":"2023-11-23T07:56:35.997009Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='TheBloke/Llama-2-7B-fp16', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"},"metadata":{}}]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:35.999072Z","iopub.execute_input":"2023-11-23T07:56:35.999414Z","iopub.status.idle":"2023-11-23T07:56:37.010670Z","shell.execute_reply.started":"2023-11-23T07:56:35.999389Z","shell.execute_reply":"2023-11-23T07:56:37.009310Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:37.014304Z","iopub.execute_input":"2023-11-23T07:56:37.015124Z","iopub.status.idle":"2023-11-23T07:56:37.027797Z","shell.execute_reply.started":"2023-11-23T07:56:37.015088Z","shell.execute_reply":"2023-11-23T07:56:37.026840Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable params: 16777216 || all params: 6755192832 || trainable%: 0.24836028248556738\n","output_type":"stream"}]},{"cell_type":"code","source":"# generation_config.__delattr__(\"top_p\")\n# generation_config","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:37.028918Z","iopub.execute_input":"2023-11-23T07:56:37.029221Z","iopub.status.idle":"2023-11-23T07:56:37.039156Z","shell.execute_reply.started":"2023-11-23T07:56:37.029196Z","shell.execute_reply":"2023-11-23T07:56:37.038283Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# generation_config.__delattr__(\"top_p\")\n# generation_config","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:37.040435Z","iopub.execute_input":"2023-11-23T07:56:37.040741Z","iopub.status.idle":"2023-11-23T07:56:37.049383Z","shell.execute_reply.started":"2023-11-23T07:56:37.040716Z","shell.execute_reply":"2023-11-23T07:56:37.048488Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode([0]))\nprint(tokenizer.unk_token)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:37.050476Z","iopub.execute_input":"2023-11-23T07:56:37.050992Z","iopub.status.idle":"2023-11-23T07:56:43.008263Z","shell.execute_reply.started":"2023-11-23T07:56:37.050965Z","shell.execute_reply":"2023-11-23T07:56:43.007312Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<unk>\n<unk>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Изменение конфигурации\ngeneration_config.temperature = 0.3\ngeneration_config.no_repeat_ngram_size = 15\ngeneration_config.do_sample = True\ngeneration_config.top_k = 40\ngeneration_config.top_p = 0.9\n# generation_config.num_beams=1\ngeneration_config.unk_token = 0\nprint(generation_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:56:43.009610Z","iopub.execute_input":"2023-11-23T07:56:43.010362Z","iopub.status.idle":"2023-11-23T07:56:43.016903Z","shell.execute_reply.started":"2023-11-23T07:56:43.010300Z","shell.execute_reply":"2023-11-23T07:56:43.016026Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_new_tokens\": 3584,\n  \"no_repeat_ngram_size\": 15,\n  \"pad_token_id\": 0,\n  \"repetition_penalty\": 1.2,\n  \"temperature\": 0.3,\n  \"top_k\": 40,\n  \"top_p\": 0.9,\n  \"unk_token\": 0\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndef get_message(inputs):\n    conversation = Conversation()\n    conversation.add_user_message(inputs)\n    prompt = conversation.get_prompt(tokenizer)\n    print('Промт', '\\n', '*'*100)\n    print(prompt)\n    print('*'*100)\n    output = generate(model, tokenizer, prompt, generation_config)\n    return output","metadata":{"execution":{"iopub.status.busy":"2023-11-23T08:03:16.238115Z","iopub.execute_input":"2023-11-23T08:03:16.238515Z","iopub.status.idle":"2023-11-23T08:03:16.245255Z","shell.execute_reply.started":"2023-11-23T08:03:16.238482Z","shell.execute_reply":"2023-11-23T08:03:16.244292Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"CPU times: user 4 µs, sys: 1 µs, total: 5 µs\nWall time: 10.5 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\np = \"\"\"\n\n\"\"\"\nget_message([p])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_message('сколько в среднем весит человек')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T08:03:35.999163Z","iopub.execute_input":"2023-11-23T08:03:35.999543Z","iopub.status.idle":"2023-11-23T08:04:30.789729Z","shell.execute_reply.started":"2023-11-23T08:03:35.999512Z","shell.execute_reply":"2023-11-23T08:04:30.788693Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Промт \n ****************************************************************************************************\n<s>system\nТы — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.</s><s>user\nсколько в среднем весит человек</s><s>bot\n****************************************************************************************************\n\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'В среднем мужчины на Земле весят 82,4 кг (181 фунта), а женщины - 69,5 кг (153 фунтов). Однако эти цифры могут сильно отличаться от страны до страны или региона. Например, в США средний вес мужчин составляет около 90-97 килограммов (около 198-212 фунтов) при росте 175 см (5\\'9\"), тогда как у женщин это 75-80 кг (165-176 фунтов) при росте 160 см (5’3\"). В Японии же средняя масса взрослых людей составляет всего 58,5 кг (129 фунты) для мужчин и 52,2 кг (115 фунтов) для женщин.</s>'"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.bos_token_id)\nprint(tokenizer.eos_token_id)\nprint(tokenizer.decode([1]))\nprint(tokenizer.decode([2]))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T05:55:59.897226Z","iopub.execute_input":"2023-11-21T05:55:59.897589Z","iopub.status.idle":"2023-11-21T05:55:59.903286Z","shell.execute_reply.started":"2023-11-21T05:55:59.897562Z","shell.execute_reply":"2023-11-21T05:55:59.902409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}