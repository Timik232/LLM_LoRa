# conf/config.yaml
#defaults:
#  - override /logging: default
defaults:
  - _self_  # Add _self_ first to include the current config in composition
  - override hydra/job_logging: disabled  # Properly override Hydra's logging
  - override hydra/hydra_logging: disabled


model:
  model_name: "Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it"
  dataset_name: "data/dataset_ru.json"
  new_model: "vikhr8b-chat-vika"
  torch_dtype: "float16"
  attn_implementation: "eager"
  train_steps: 60
  outfile: "model-game_v5.gguf"
  quant_postfix: "_q4"
  qtype: "q4_1"
  version: "v5"
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

training:
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  num_train_epochs: 0.1
  eval_steps: 20
  logging_steps: 5
  warmup_steps: 10
  learning_rate: 2e-5
  fp16: false
  bf16: false
  weight_decay: 0.05
  max_seq_length: 2048
  optim: "paged_adamw_8bit"
  neftune_noise_alpha: 0
  gradient_checkpointing: true
  save_total_limit: 5
  load_best: False

paths:
  data_dir: "data"
  merged_model_path: "merged_model_fp16"
  output_dir: "models"
#  venv_python_path: "T:/projects/LLM_LoRa/venv/Scripts/python.exe"
  venv_python_path: "python"
  llama_cpp_dir: "../llama.cpp"
#  lmstudio_path: "T:/lm-studio/models/game-model"
  final_weights_path: "models"
  quantized_path: "build/bin/llama-quantize"

other:
  cutoff_len: 2048

hydra:
  run:
    dir: .
  job:
    chdir: true  # Address future Hydra working dir change warning
